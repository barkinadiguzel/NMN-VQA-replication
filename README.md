# ğŸ¨ NMN-VQA-Replication â€” Neural Module Networks for Visual Question Answering

This repository provides a **PyTorch-based replication** of  
**Neural Module Networks (NMN) for Visual Question Answering â€” Andreas et al., 2016/2017**.

The focus is on **faithfully reproducing the NMN pipeline**  
with a clean, modular, and research-oriented implementation.

- Decomposes **questions into linguistic substructures** ğŸ“  
- Dynamically assembles **neural modules** for reasoning ğŸ§©  
- Integrates **visual features + attention + text context** ğŸ¤  

**Paper reference:** [Deep Compositional Question Answering with Neural Module Networks â€” Andreas et al., 2016](https://arxiv.org/abs/1511.02799) ğŸ“„

---

## ğŸ§  Overview â€” NMN Visual Question Answering Pipeline

![NMN Overview](images/figmix.jpg)

The core idea:

> Intelligence emerges from **compositional reasoning over image and question**, not from monolithic predictions.

Instead of directly mapping  
$(image, question) \rightarrow answer$,  
the model is structured as:

$$
x \;,\; w \;\longrightarrow\; \text{NMN Layout} \;\longrightarrow\; \{modules\} \;\longrightarrow\; y
$$

Where:  
- $x$ = input image  
- $w$ = question string  
- $modules$ = reusable neural modules (attend, re-attend, combine, classify, measure)  
- $y$ = predicted answer  

The model dynamically composes modules based on **question structure**, then fuses outputs with an optional LSTM question encoder.

---

## ğŸ‘ Vision Encoder â€” Image to Attention Maps

Given an input image $x$, a CNN backbone produces a spatial feature map:

$$F(x) \in \mathbb{R}^{H \times W \times C}$$

Attention modules (attend[object], attend[color], etc.) produce **unnormalized heatmaps**:

```math
A_c = attend[c](F(x))
```

Where $c$ is the concept (dog, red, tie, etc.).  

Higher-level re-attend and combine modules allow **spatial shifts** and **logical composition**:

```math
A' = re\_attend[above](A) \quad,\quad A'' = combine[and](A_1, A_2)
```

---

## ğŸ¯ Classification & Measurement â€” Final Answer

Classification modules map attention maps to label distributions:

$$
p_\text{class} = classify[c](F(x), A)
$$

Measurement modules operate on attention maps alone (e.g., yes/no, count):

```math
p_\text{measure} = measure[c](A)
```

The optional LSTM question encoder produces a **textual context distribution**:

$$
p_\text{LSTM} = LSTM(w)
$$

Final prediction is a **fusion of NMN and LSTM outputs**:

$$
p_\text{final}(y) \propto \sqrt{p_\text{NMN}(y) \cdot p_\text{LSTM}(y)}
$$

---

## ğŸ§© What the Model Learns

- Visual object detection via attention  
- Compositional reasoning over objects and attributes  
- Spatial and logical relationships  
- Attribute binding & scene structure  
- Common-sense knowledge via LSTM fusion  

---

## ğŸ“¦ Repository Structure

```bash
NMN-VQA-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ cnn_encoder.py         # Image â†’ feature map
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ attend.py              # Image â†’ Attention (attend[dog], attend[red], etc.)
â”‚   â”‚   â”œâ”€â”€ re_attend.py           # Attention â†’ Attention (re-attend[above], re-attend[not])
â”‚   â”‚   â”œâ”€â”€ combine.py             # Attention Ã— Attention â†’ Attention
â”‚   â”‚   â”œâ”€â”€ classify.py            # Image Ã— Attention â†’ Label (classify[color], classify[where])
â”‚   â”‚   â””â”€â”€ measure.py             # Attention â†’ Label (yes/no, count)
â”‚   â”‚
â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â””â”€â”€ parser_to_layout.py    # Question string â†’ NMN network layout
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ nmn_pipeline.py        # Image + Question â†’ NMN modules â†’ Final Answer
â”‚   â”‚
â”‚   â”œâ”€â”€ question_encoder/
â”‚   â”‚   â””â”€â”€ lstm_encoder.py        # Optional: LSTM question encoder for context & common sense
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ loss.py                # VQA-specific loss (cross-entropy)
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg               
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
